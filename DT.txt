A Decision Tree is a simple representation for classifying examples. It is a Supervised Machine Learning where the data is continuously split according to a certain parameter.Decision Tree consists of Nodes which Test for the value of a certain attribute.Branch is Correspond to the outcome of a test and connect to the next node or leaf.
Leaf/Terminal nodes that predict the outcome.
two main types of Decision Trees:
Classification Trees.
Regression Trees.
1. Classification trees (categ.) :
What we’ve seen above is an example of classification tree, where the outcome was a variable like ‘fit’ or ‘unfit’. Here the decision variable is Categorical/ discrete.
This is an iterative process of splitting the data into partitions, and then splitting it up further on each of the branches.
2. Regression trees (Cont.) :
Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. (e.g. the price of a house)
Creation of Decision Tree :
In this method a set of training examples is broken down into smaller and smaller subsets.The key idea is to use a decision tree to partition the data space into cluster (or dense) regions and empty (or sparse) regions.
Decision Tree Classifier
Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest information gain (IG) (reduction in uncertainty towards the final decision).
In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are pure. This means that the samples at each leaf node all belong to the same class.
Advantages of Classification with Decision Trees:
Inexpensive to construct.
Extremely fast at classifying unknown records.
Easy to interpret for small-sized trees
Accuracy comparable to other classification techniques for many simple data sets.
Excludes unimportant features.
Disadvantages of Classification with Decision Trees:
Easy to overfit.
Decision tree models are often biased toward splits on features having a large number of levels.
Small changes in the training data can result in large changes to decision logic.
Large trees can be difficult to interpret.