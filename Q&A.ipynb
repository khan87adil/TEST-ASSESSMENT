{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "huggingface-hub 0.0.12 requires packaging>=20.9, but you'll have packaging 20.4 which is incompatible.\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/spacy/\n",
      "ERROR: Could not find a version that satisfies the requirement spacy==2.1.3 (from versions: none)\n",
      "ERROR: No matching distribution found for spacy==2.1.3\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/transformers/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/transformers/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/transformers/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/transformers/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/transformers/\n",
      "ERROR: Could not find a version that satisfies the requirement transformers==2.2.2 (from versions: none)\n",
      "ERROR: No matching distribution found for transformers==2.2.2\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/neuralcoref/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/neuralcoref/\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\ANACONDA\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\91986\\\\AppData\\\\Local\\\\Temp\\\\pip-install-q35er30d\\\\neuralcoref\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\91986\\\\AppData\\\\Local\\\\Temp\\\\pip-install-q35er30d\\\\neuralcoref\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\91986\\AppData\\Local\\Temp\\pip-wheel-lof63a65'\n",
      "       cwd: C:\\Users\\91986\\AppData\\Local\\Temp\\pip-install-q35er30d\\neuralcoref\\\n",
      "  Complete output (25 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  creating build\\lib.win-amd64-3.8\\neuralcoref\n",
      "  copying neuralcoref\\file_utils.py -> build\\lib.win-amd64-3.8\\neuralcoref\n",
      "  copying neuralcoref\\__init__.py -> build\\lib.win-amd64-3.8\\neuralcoref\n",
      "  creating build\\lib.win-amd64-3.8\\neuralcoref\\tests\n",
      "  copying neuralcoref\\tests\\test_neuralcoref.py -> build\\lib.win-amd64-3.8\\neuralcoref\\tests\n",
      "  copying neuralcoref\\tests\\__init__.py -> build\\lib.win-amd64-3.8\\neuralcoref\\tests\n",
      "  creating build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\algorithm.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\compat.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\conllparser.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\dataset.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\document.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\evaluator.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\learn.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\model.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\utils.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\__init__.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "  running build_ext\n",
      "  building 'neuralcoref.neuralcoref' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for neuralcoref\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\ANACONDA\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\91986\\\\AppData\\\\Local\\\\Temp\\\\pip-install-q35er30d\\\\neuralcoref\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\91986\\\\AppData\\\\Local\\\\Temp\\\\pip-install-q35er30d\\\\neuralcoref\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\91986\\AppData\\Local\\Temp\\pip-record-0_m8dvzq\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\ANACONDA\\Include\\neuralcoref'\n",
      "         cwd: C:\\Users\\91986\\AppData\\Local\\Temp\\pip-install-q35er30d\\neuralcoref\\\n",
      "    Complete output (25 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.8\n",
      "    creating build\\lib.win-amd64-3.8\\neuralcoref\n",
      "    copying neuralcoref\\file_utils.py -> build\\lib.win-amd64-3.8\\neuralcoref\n",
      "    copying neuralcoref\\__init__.py -> build\\lib.win-amd64-3.8\\neuralcoref\n",
      "    creating build\\lib.win-amd64-3.8\\neuralcoref\\tests\n",
      "    copying neuralcoref\\tests\\test_neuralcoref.py -> build\\lib.win-amd64-3.8\\neuralcoref\\tests\n",
      "    copying neuralcoref\\tests\\__init__.py -> build\\lib.win-amd64-3.8\\neuralcoref\\tests\n",
      "    creating build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\algorithm.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\compat.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\conllparser.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\dataset.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\document.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\evaluator.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\learn.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\model.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\utils.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\__init__.py -> build\\lib.win-amd64-3.8\\neuralcoref\\train\n",
      "    running build_ext\n",
      "    building 'neuralcoref.neuralcoref' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\ANACONDA\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\91986\\\\AppData\\\\Local\\\\Temp\\\\pip-install-q35er30d\\\\neuralcoref\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\91986\\\\AppData\\\\Local\\\\Temp\\\\pip-install-q35er30d\\\\neuralcoref\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\91986\\AppData\\Local\\Temp\\pip-record-0_m8dvzq\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\ANACONDA\\Include\\neuralcoref' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q bert-extractive-summarizer\n",
    "!pip install -q spacy==2.1.3\n",
    "!pip install -q transformers==2.2.2\n",
    "!pip install -q neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\anaconda\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\anaconda\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/tansformers/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/tansformers/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/tansformers/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/tansformers/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/tansformers/\n",
      "ERROR: Could not find a version that satisfies the requirement tansformers (from versions: none)\n",
      "ERROR: No matching distribution found for tansformers\n"
     ]
    }
   ],
   "source": [
    "pip install tansformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython in c:\\anaconda\\lib\\site-packages (7.19.0)\n",
      "Requirement already satisfied: decorator in c:\\anaconda\\lib\\site-packages (from ipython) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in c:\\anaconda\\lib\\site-packages (from ipython) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\anaconda\\lib\\site-packages (from ipython) (50.3.1.post20201107)\n",
      "Requirement already satisfied: backcall in c:\\anaconda\\lib\\site-packages (from ipython) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\anaconda\\lib\\site-packages (from ipython) (2.0.10)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\anaconda\\lib\\site-packages (from ipython) (0.4.4)\n",
      "\n",
      "Requirement already satisfied: pygments in c:\\anaconda\\lib\\site-packages (from ipython) (2.7.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\anaconda\\lib\\site-packages (from ipython) (5.0.5)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\anaconda\\lib\\site-packages (from ipython) (0.17.1)\n",
      "Requirement already satisfied: wcwidth in c:\\anaconda\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.5)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\anaconda\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\anaconda\\lib\\site-packages (from traitlets>=4.2->ipython) (0.2.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\anaconda\\lib\\site-packages (from jedi>=0.10->ipython) (0.7.0)\n"
     ]
    }
   ],
   "source": [
    "pip install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cbada47398466bad82ee101ad5e9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5cfa5f6716458aad79e9e63375b97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5655f0826c784f5ba421d48ec9b860e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d53a50ff7d84431aacbbbe74017803f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarization=pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"12 Statistics Concepts You Must Know For Your Next Data Science Interview\n",
    "Ace your next Data Science Interview with confidence\n",
    "Ananya Bhattacharyya\n",
    "Ananya Bhattacharyya\n",
    "\n",
    "Aug 8·11 min read\n",
    "\n",
    "\n",
    "Statistics provides the tools and methods to find structure and meaningful insights into our data, and also helps us to quantify the embedded uncertainty and hence, having a good foundation of Statistics is crucial for any Data Scientist. So, for any Data Science interview, we get tested based on our knowledge of Statistics.\n",
    "\n",
    "Photo by Myriam Jessier on Unsplash\n",
    "Here in this article, I have compiled 12 statistical concepts that I found to be very useful for cracking interviews. So, here I am going to go over those 12 concepts and explain what they’re all about.\n",
    "Conditional Probability and Bayes’ Theorem\n",
    "For any two events A and B, P(A|B) represents the conditional probability of occurrence of event A given that event B has already occurred. The formula for conditional probability is given by the following equation –\n",
    "\n",
    "In continuation to the discussion of conditional probability, revising our prior probability of an event when new information becomes available is a crucial phase and that is where Bayes’ Theorem becomes useful. The following mathematical equation sums up Bayes’ Theorem.\n",
    "\n",
    "In this equation A is an event and B is empirical evidence or information received from data. So, P(A) is the prior probability of event A and P(B) is the probability of event B based on evidence from data, and P(B|A) is known as the likelihood. So, Bayes’ Theorem gives us the probability of an event based on our prior knowledge about the event and updates that conditional probability when we get some new information about the same.\n",
    "A very easy example of Bayes’ theorem can be to predict the probability of raining on a particular day given that the morning was cloudy. Suppose, the probability of raining i.e., P(Rain) is 10% on a day in June and the probability that the morning was cloudy given it rained i.e., P(Cloud|Rain) is 50%. Additionally, the probability of a cloudy morning in any day in June i.e., P(Cloud) is 40%, then applying Bayes’ theorem we can conclude that the probability that it will rain today given that it was cloudy in the morning is:\n",
    "\n",
    "2. Sampling technique\n",
    "If we call all elements in a group as the Population, then a subset of that population is called a sample. The total number of observations or individuals in that sample is known as the sample size. Any statistical constants of the population, for example, the descriptive measures like mean, the variance of any characteristic of the population are known as parameters. The same statistical measure will be known as a statistic when it is computed based on sample observations. Now the method of choosing the sample from a population can be of two types- probabilistic and non-probabilistic.\n",
    "Probabilistic Sampling Methods\n",
    "a. Simple random sampling\n",
    "In simple random sampling, each sample is equally likely to be selected from the population. This technique reduces selection bias. However, a disadvantage of this technique is that we may not be able to select samples based on any particular characteristic, which is uncommon.\n",
    "Example — An example of simple random sampling can be of choosing 10 students from a class of 100 based on a lottery.\n",
    "b. Systematic sampling\n",
    "In Systematic sampling, samples are collected at regular and specified intervals. The advantage of systematic sampling is that it helps to maintain an adequate sampling size.\n",
    "An example of systematic sampling can be of choosing every 10th customer in a grocery store for a customer satisfaction survey.\n",
    "c. Stratified sampling\n",
    "In stratified sampling, the entire population is divided into various subgroups, also known as, strata which have similar characteristics. The main objective of this sampling is that to have representation from all the subgroups which show variability in the characteristics of interest. .\n",
    "For example, if we are interested to know about people’s opinion on any particular topic in India, we can divide the population based on cities, gender and age group. These sub-populations based on the chosen characteristics are called strata and each of the samples in a strata are chosen based on probability sampling method e.g., simple random sampling.\n",
    "Non-Probabilistic Sampling Methods\n",
    "a. Convenience sampling\n",
    "In Convenience sampling, participants are chosen based on their availability and willingness to take part in the study. However, this technique can suffer from volunteer bias which can be a risk of all non-probability-based sampling methods because of the samples choosing themselves instead of getting randomly picked.\n",
    "b. Quota sampling\n",
    "Quota sampling is quite popular in market research. In this case, an interviewer simply chooses samples based on a required quota for any research/study. For example, a smartphone company might want to investigate what age group using which brand of phone in a city. Then they apply quotas e.g., 100 people from each of these age groups e.g., 21–30, 31–40, 41–50 or they can further impose quotas based on gender e.g., 50 male and 50 female from each age group. They will choose the samples based on ease of access and their research budget. However, as mentioned earlier being a non-probabilistic method, quota sampling may still have voluntary bias.\n",
    "c. Purposive Sampling\n",
    "In Purposive sampling, the Researchers apply their expert knowledge of the target population in order to select the “representative” sample in a non-random manner. Purposive sampling is definitely time-and cost-effective but in addition to volunteer bias, this type of sampling can cause judgment bias/error by the researcher in choosing participants.\n",
    "3. Probability Distributions\n",
    "Probability distributions are crucial for understanding any data structure. There are primarily two types of distributions — Discrete and Continuous.\n",
    "a. Discrete probability distributions\n",
    "These distributions model the probabilities of random variables that have discrete values as outcomes. For example, if X is a discrete random variable describing the number of heads in a toss, we can repeat the toss many times and record the probability of each possible outcome. If we represent this in a form of function then it will be called a “probability mass function (PMF)” which defines the probability of the discrete random variable X taking on a particular value x. Examples of such distributions are Bernoulli, Binomial, Poisson.\n",
    "b. Continuous probability distributions: On the other hand, the continuous probability distributions model the probabilities of random variables that can have any possible outcome which is essentially continuous in nature. For example, the possible values for the random variable X that represents weights of citizens in a town can have any value like 54.5, 47.2, 60.3, etc. Examples can be Normal, Student’s T, Chi-square, Exponential distribution, etc.\n",
    "4. Hypothesis Testing\n",
    "We use a Hypothesis to make a claim or assumption about the population parameter. Now in order to verify our claim on the basis of the sample data that we collected we create two hypotheses — null and alternative.\n",
    "The null hypothesis reflects a researcher’s neutral opinion about the outcome of the hypothesis test. It basically states the default situation e.g., in a medicine efficacy experiment, the null hypothesis can be that there is no difference in cure rate between the patients who got the medicine and the group of patients who got a placebo. The exact opposite statement of the null hypothesis will be the alternative hypothesis. Alternative hypothesis always challenges the null hypothesis. The decision to accept or reject the null hypothesis is done based on the sample data.\n",
    "5. Confidence level\n",
    "Continuing with the hypothesis testing concepts it is very important to understand what the confidence level is. The confidence level in hypothesis testing is the probability of not rejecting the null hypothesis when the null hypothesis is actually true. It conveys how sure we are about getting the same results if we repeat the experiment again. So, we construct a confidence interval which is an interval estimation of a parameter generated from statistical inference. This interval is calculated by the following formula:\n",
    "[point estimation ± critical value based on sample distribution at chosen confidence level*standard deviation of sample]\n",
    "For example, if we have a sampling distribution with mean 60 and standard deviation 5, then the 95% confidence interval will range from (60–1.96*5) i.e., 50.2 and (60 +1.96*5) i.e., 69.8. Here, 1.96 is the critical value which comes from the fact that for a normal distribution, 95% area lies within a standard deviation of 1.96\n",
    "We can never be 100 % confident in statistics as we will always have some uncertainties. Hence, we mostly come across 99%, 95%, or 90% confidence levels in statistical studies.\n",
    "6. p values and level of significance\n",
    "For any statistical inferential study we need to make a decision about rejecting/accepting the null hypothesis and that decision is made based on the observed values of the randomized sample. However, there is always a probability that the conclusion that we are drawing from the samples about the population is wrong. The error associated with rejecting a null hypothesis when it is actually true is known as Type I error and the error associated with accepting the null hypothesis when it is false is called Type II error. Now, let’s assume, the probability of committing a type I error is some number, α, which is the level of significance any researcher sets for his study. An α of 5% indicates that we are willing to accept a 5% chance that we are wrong when we reject the null hypothesis. On the other hand, the p-value is the observed significance level which gives us the probability of obtaining an effect as extreme as the one calculated from our sample data when the null hypothesis is true.\n",
    "7. A/B testing\n",
    "A/B testing is a randomized experiment with two groups, A and B. In a very simple language, it is a way to compare two variants of a single variable to find out which variant performs better than the other in a controlled environment. This technique is often used for marketing strategies to improve customer satisfaction.\n",
    "For example, two marketing emails with different call to actions can be sent to the 30% of the total customer base of a company just to test which one has a higher clicking rate. The marketing team then wait for a few hours and they choose to send the email with more clicks to rest of the 70% customers\n",
    "8. Z test and t-test\n",
    "Both z test and t-test are very useful in hypothesis testing. Z tests are typically used when we want to understand if two sample means are significantly different from each other when the sample size is large i.e., n ≥ 30, and also the population standard deviation is known to us. On the other hand, t-tests can be used to compare two sample means for a small sample size n >0 and even when the population standard deviation is not known to us. t-test follows student’s t- distribution whereas the z test assumes that the sample distribution is normal. In the case of a large sample the Z test and t-Test will tend to give us the same results as for a large enough sample size, t-distribution approaches the normal distribution and the difference between the z score and t score becomes negligible. Now, t-tests can be used to perform a One-Sample t-test when we want to understand if a sample mean is significantly different than the population average. Since we don’t know the population standard deviation in the case of a t-test we use the sample standard deviation.\n",
    "9. Difference between Covariance and Correlation\n",
    "The covariance measure gives us the direction of the linear relationship between two variables. On the other hand, Correlation gives us both the strength and direction of the relationship between those two variables. If we think about the mathematical formula, we can calculate the correlation coefficient of two variables by dividing the covariance of these variables by the product of the standard deviations of the same variables. So, correlation values are standardized. Hence, the Correlation coefficient lies between -1 to +1 but covariance can lie between -∞ to +∞.\n",
    "10. Linear regression vs logistic regression\n",
    "Linear Regression is commonly used when the dependent variable is continuous. One of the key assumptions of linear regression is that there is a linear relationship between dependent and independent variables. On the other hand, Logistic Regression is used when the dependent variable is binary. It predicts the probability of occurrence of an event by fitting the data into a logit function. For Logistic regression, we do not need a linear relationship between the dependent and independent variables.\n",
    "Linear Regression fits a straight line in the data while Logistic Regression fits a curve to the data. Linear regression assumes the Gaussian (or normal) distribution of the prediction error. However, the dependent variable or independent variables of the linear regression-based model need not follow the normal distribution. Sometimes when the training data has a few outliers or the dependent variable is too skewed, it is possible that the resulting error in the model also does not follow the normal distribution. Logistic regression assumes the binomial distribution of the dependent variable.\n",
    "11. Central limit theorem and Law of large numbers\n",
    "The central limit theorem states that regardless of a particular variable’s distribution in the population when we are collecting samples from the population the sampling distribution of the variable mean will approach the normal distribution as the sample size increases. The Central Limit Theorem is very important in statistics for two main reasons — the normality assumption is crucial for parametric hypothesis testing and the precision of the estimates. Often in real-life data, we come across the non-normal distribution. So, we might get worried that the hypothesis tests that we are conducting or the parameter estimates we are getting are not giving us accurate results. However, if we have a large sample size (>30), then CLT allows us to use the tests and the estimates even when the data is not normally distributed.\n",
    "On the other hand, as per the Law of Large Numbers as we increase the number of trials in an experiment the average result of the trials will eventually approach the true population average. For example, we are more likely to see Heads coming up half of the time when we toss a fair coin 1000 times as compared to tossing the same coin only 10 times.\n",
    "12. Maximum Likelihood Estimation (MLE)\n",
    "Maximum Likelihood Estimation involves estimating the parameter by maximizing the likelihood function to find the parameters that best explain the observed data. MLE is such a predictive modeling framework where model parameters are found through an optimization problem. Here, the likelihood function p(y| θ ) describes the likelihood of observing data y given parameter θ. We solve the optimization problem by maximizing the set of parameters (theta) that gives us the largest likelihood value. MLEs work well with large data sizes and gives us unbiased minimum variance estimators.\n",
    "EndNote:\n",
    "Thanks for reading!\n",
    "Having a very strong understanding of Statistics is a prerequisite for being successful as a Data Scientist. I hope this article will help you to prepare for your next Data Science interview. The 12 concepts that I have discussed here will set up the focus areas for you to learn further.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=summarization(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'EDUCATIONData Science Masterâ€TMs Program, Mumbai . Jan 2020 â€“ July 2021GreyAtom School of Data Science B.E. in Automobile Engg., Mumbai. Aug 2020 . Dec 2020Developing Software product such as AI/ML powered Virtual Assistants on Cloud (Azure, AWS, GCP, IBM etc.. identifying the trends or Volatility of Stock Price. Trade prediction using classification & Diversification analysis using clustering .'}]\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"c.txt\", 'r') as file:\n",
    "    data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(\"\\ufeff\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EDUCATIONData Science Masterâ€™s Program, Mumbai. Jan 2020 â€“ July 2021GreyAtom School of Data Scie'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-e7c35794913d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\summarizer\\model_processors.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, custom_model, custom_tokenizer, hidden, reduce_option, sentence_handler, random_state, hidden_concat)\u001b[0m\n\u001b[0;32m    332\u001b[0m         \"\"\"\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         super(Summarizer, self).__init__(\n\u001b[0m\u001b[0;32m    335\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_option\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_concat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         )\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\summarizer\\model_processors.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, custom_model, custom_tokenizer, hidden, reduce_option, sentence_handler, random_state, hidden_concat)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m     52\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertParent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_tokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_option\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_option\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\summarizer\\bert_parent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, custom_model, custom_tokenizer)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             self.model = base_model.from_pretrained(\n\u001b[0m\u001b[0;32m     47\u001b[0m                 model, output_hidden_states=True).to(self.device)\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\transformers\\utils\\dummy_pt_objects.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m         \u001b[0mrequires_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"torch\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANACONDA\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__name__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBACKENDS_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBACKENDS_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-a8194ba030e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "result = model(data, num_sentences=5, min_length=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
